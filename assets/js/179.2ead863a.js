(window.webpackJsonp=window.webpackJsonp||[]).push([[179],{496:function(e,t,a){"use strict";a.r(t);var n=a(15),r=Object(n.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding"}},[e._v("#")]),e._v(" BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding")]),e._v(" "),a("h2",{attrs:{id:"model-architecture"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#model-architecture"}},[e._v("#")]),e._v(" Model Architecture")]),e._v(" "),a("p",[e._v("BERT’s model architecture is a multi-layer bidirectional Transformer encoder。")]),e._v(" "),a("h2",{attrs:{id:"input-output-representations"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#input-output-representations"}},[e._v("#")]),e._v(" Input/Output Representations")]),e._v(" "),a("p",[e._v("For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings。")]),e._v(" "),a("center",[a("img",{staticStyle:{zoom:"50%"},attrs:{src:"/img/AI/NLP/BERT/1.png",alt:"image-20210820161507599"}})]),e._v(" "),a("h2",{attrs:{id:"pre-training-bert"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pre-training-bert"}},[e._v("#")]),e._v(" Pre-training BERT")]),e._v(" "),a("h3",{attrs:{id:"task-1-masked-lm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#task-1-masked-lm"}},[e._v("#")]),e._v(" Task #1: Masked LM")]),e._v(" "),a("p",[e._v("In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens.")]),e._v(" "),a("h3",{attrs:{id:"task-2-next-sentence-prediction-nsp"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#task-2-next-sentence-prediction-nsp"}},[e._v("#")]),e._v(" Task #2: Next Sentence Prediction (NSP)")]),e._v(" "),a("p",[e._v("In order to train a model that understands sentence relationships, we pre-train for a binarized "),a("em",[e._v("next sentence prediction")]),e._v(" task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).")]),e._v(" "),a("center",[a("img",{staticStyle:{zoom:"67%"},attrs:{src:"/img/AI/NLP/BERT/2.png",alt:"image-20210820162101296"}})]),e._v(" "),a("p",[e._v('"NSP"：Next Sentence Prediction')])],1)}),[],!1,null,null,null);t.default=r.exports}}]);