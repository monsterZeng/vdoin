(window.webpackJsonp=window.webpackJsonp||[]).push([[207],{469:function(t,e,r){"use strict";r.r(e);var n=r(15),a=Object(n.a)({},(function(){var t=this,e=t.$createElement,r=t._self._c||e;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"k近邻"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#k近邻"}},[t._v("#")]),t._v(" K近邻")]),t._v(" "),r("p",[t._v("基于实例的学习，是一种非显示的学习过程，也就是没有训练阶段，直接让新样本直接与训练集做分类或者是回归预测。")]),t._v(" "),r("p",[t._v("所谓最近邻，就是首先选取一个阈值为K，对在阈值范围内离测试样本最近的点进行投票，票数多的类别就是这个测试样本的类别，这是分类问题。那么回归问题也同理，对在阈值范围内离测试样本最近的点取均值，那么这个值就是这个样本点的预测值。")]),t._v(" "),r("p",[t._v("K值得选取：")]),t._v(" "),r("ul",[r("li",[t._v("如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”的近似误差会减小，只有与输入实例较近的训练实例才会对预测结果起作用。但缺点是“学习”的误差估计会增大，预测结果会对近邻的实例点非常敏感，如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，K值得减小就意味着整体模型变得复杂，易发生过拟合。")]),t._v(" "),r("li",[t._v("选择较大的K值，就相当于用较大领域中的训练实例进行预测。优点是可以减少学习的估计误差，但缺点是近似误差会增大。这时与输入实例较远的训练实例也会对预测起作用，使预测发生误差。K值增大会使模型变得简单。")])]),t._v(" "),r("p",[t._v("最近邻算法的优点：思想简单，可用与分类与回归，准确度高，对噪声不敏感。")]),t._v(" "),r("p",[t._v("缺点：计算量大，难解决样本不平衡问题，需要大量内存。")]),t._v(" "),r("p",[r("a",{attrs:{href:"http://sofasofa.io/forum_main_post.php?postid=1001551",target:"_blank",rel:"noopener noreferrer"}},[t._v("K近邻算法的缺点"),r("OutboundLink")],1)])])}),[],!1,null,null,null);e.default=a.exports}}]);