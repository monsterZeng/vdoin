(window.webpackJsonp=window.webpackJsonp||[]).push([[169],{506:function(t,s,a){"use strict";a.r(s);var i=a(15),e=Object(i.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"mixed-precision-training"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mixed-precision-training"}},[t._v("#")]),t._v(" MIXED PRECISION TRAINING")]),t._v(" "),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"I:\\Storage\\project\\vuepress-theme-vdoing\\docs\\.vuepress\\public\\img\\AI\\ML\\Optimization\\Mix precision training\\1.png",alt:"image-20210818094357191"}}),t._v(" "),a("p",[t._v("We introduce the key techniques for training with FP16:")]),t._v(" "),a("ul",[a("li",[t._v("single-precision master weights and updates")]),t._v(" "),a("li",[t._v("loss-scaling")]),t._v(" "),a("li",[t._v("accumulating FP16 products into FP32")])]),t._v(" "),a("h2",{attrs:{id:"fp32-master-copy-of-weights"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fp32-master-copy-of-weights"}},[t._v("#")]),t._v(" FP32 MASTER COPY OF  WEIGHTS")]),t._v(" "),a("p",[t._v("在混合精确训练中，网络的权重、激活函数和梯度以FP16的方式存储。为了匹配FP32网络的准确性，拷贝一份FP32的网络权重参数，并且使用优化器用梯度去更新它。")]),t._v(" "),a("p",[t._v("FP16的拷贝被使用在前向传播和后向传播，FP16的存储空间和位宽都是FP32的一半。")]),t._v(" "),a("p",[t._v("虽然对FP32主权重的需求不是普遍的，但有两个可能的原因导致许多网络需要它。")]),t._v(" "),a("ul",[a("li",[t._v("一种解释是更新(权重梯度乘以学习率)变得太小，无法在FP16中表示——任何小于"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",[a("semantics",[a("mrow",[a("msup",[a("mn",[t._v("2")]),a("mrow",[a("mo",[t._v("−")]),a("mn",[t._v("2")]),a("mn",[t._v("4")])],1)],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("2^{-24}")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"strut",staticStyle:{height:"0.8141079999999999em"}}),a("span",{staticClass:"strut bottom",staticStyle:{height:"0.8141079999999999em","vertical-align":"0em"}}),a("span",{staticClass:"base textstyle uncramped"},[a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathrm"},[t._v("2")]),a("span",{staticClass:"vlist"},[a("span",{staticStyle:{top:"-0.363em","margin-right":"0.05em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),a("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[a("span",{staticClass:"mord scriptstyle uncramped"},[a("span",{staticClass:"mord"},[t._v("−")]),a("span",{staticClass:"mord mathrm"},[t._v("2")]),a("span",{staticClass:"mord mathrm"},[t._v("4")])])])]),a("span",{staticClass:"baseline-fix"},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("的值在FP16中为零，所以导致无法更新权重。")]),t._v(" "),a("li",[t._v("权重值对于权重更新的比例非常大。")])]),t._v(" "),a("h2",{attrs:{id:"loss-scaling"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#loss-scaling"}},[t._v("#")]),t._v(" LOSS SCALING")]),t._v(" "),a("p",[t._v("作者收集了FP32训练的激活函数的梯度，发现很大一部分训练到后期，梯度会很小，为了能以FP16的方式有效保存，将loss进行缩放，loss缩放了，根据链式法则，那么梯度也会跟着缩放，可以有效的保存梯度。")]),t._v(" "),a("p",[t._v("在权重更新之前，权重梯度必须是无比例的，以保持FP32训练中的更新幅度。最简单的做法是在反向传递之后，但在梯度裁剪或任何其他与梯度相关的计算之前，执行此缩放操作，确保无需调整超参数(如梯度裁剪阈值、权重衰减等)。")]),t._v(" "),a("h2",{attrs:{id:"arithmetic-precision"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arithmetic-precision"}},[t._v("#")]),t._v(" ARITHMETIC PRECISION")]),t._v(" "),a("p",[t._v("总的来说，神经网络算法可分为三类:向量点积、Reduction和逐点运算。当涉及到降低精度的运算时，这些类别受益于不同的处理。为了保持模型的准确性，我们发现有些网络需要这样做:FP16向量点积将部分积 累加为FP32值，在写入内存之前将FP32值转换为FP16。如果FP32中没有这种积累，一些FP16模型就无法匹配基线模型的准确性。")]),t._v(" "),a("p",[t._v("在FP32中应该进行大的reduction(向量元素之间的总和)。这种reduction主要出现在batch normalization中的累加统计和softmax layers。在我们的实现中，这两种层类型仍然从内存中读写FP16张量，执行FP32中的算法。这并没有减慢训练过程，因为这些层是内存带宽有限(IO)的，对算术速度不敏感。")]),t._v(" "),a("p",[t._v("点运算，如非线性和元素矩阵乘积，是内存带宽有限的。因为算术精度也不会影响这些操作的速度，使用FP16或FP32都可以。")])])}),[],!1,null,null,null);s.default=e.exports}}]);