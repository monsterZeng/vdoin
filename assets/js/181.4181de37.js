(window.webpackJsonp=window.webpackJsonp||[]).push([[181],{494:function(t,a,e){"use strict";e.r(a);var r=e(15),n=Object(r.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"roberta-a-robustly-optimized-bert-pretraining-approach"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#roberta-a-robustly-optimized-bert-pretraining-approach"}},[t._v("#")]),t._v(" RoBERTa: A Robustly Optimized BERT Pretraining Approach")]),t._v(" "),e("h2",{attrs:{id:"改进"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#改进"}},[t._v("#")]),t._v(" 改进")]),t._v(" "),e("h3",{attrs:{id:"dynamic-masking"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#dynamic-masking"}},[t._v("#")]),t._v(" Dynamic Masking")]),t._v(" "),e("p",[t._v("To avoid using the same mask for each training instance in every epoch, training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training. Thus, each training sequence was seen with the same mask four times during training.")]),t._v(" "),e("h3",{attrs:{id:"model-input-format-and-next-sentence-prediction"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#model-input-format-and-next-sentence-prediction"}},[t._v("#")]),t._v(" Model Input Format and Next Sentence Prediction")]),t._v(" "),e("p",[t._v("removing the NSP loss matches or slightly improves downstream task performance.")]),t._v(" "),e("h3",{attrs:{id:"training-with-large-batches"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#training-with-large-batches"}},[t._v("#")]),t._v(" Training with large batches")]),t._v(" "),e("h3",{attrs:{id:"text-encoding"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#text-encoding"}},[t._v("#")]),t._v(" Text Encoding")])])}),[],!1,null,null,null);a.default=n.exports}}]);