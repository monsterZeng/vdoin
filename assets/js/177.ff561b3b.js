(window.webpackJsonp=window.webpackJsonp||[]).push([[177],{498:function(t,s,a){"use strict";a.r(s);var e=a(15),r=Object(e.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("center",[a("img",{staticStyle:{zoom:"50%"},attrs:{src:"/img/AI/NLP/Transformer/1.png",alt:"image-20210820110017368"}})]),t._v(" "),a("h2",{attrs:{id:"encoder-and-decoder-stacks"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#encoder-and-decoder-stacks"}},[t._v("#")]),t._v(" Encoder and Decoder Stacks")]),t._v(" "),a("h3",{attrs:{id:"encoder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#encoder"}},[t._v("#")]),t._v(" Encoder")]),t._v(" "),a("p",[t._v("编码器由N = 6个相同层的堆栈组成。每一层有两个子层。第一种是多头自注意机制，第二种是简单的全连接前馈网络。我们在两个子层上使用残差连接，然后进行layer normalization。即每个子层的输出为LayerNorm(x +子层(x))，其中子层(x)是由子层本身实现的函数。为了方便这些剩余连接，模型中的所有子层以及嵌入层都会产生dimension = 512的输出。")]),t._v(" "),a("h3",{attrs:{id:"decoder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#decoder"}},[t._v("#")]),t._v(" Decoder")]),t._v(" "),a("p",[t._v("解码器也由N = 6个相同层的堆栈组成。除每个编码器层中的两个子层外，解码器还插入第三个子层，该子层对编码器堆栈的输出执行多头注意。与编码器类似，我们在每个子层周围使用残差连接，然后进行层归一化。还修改了解码器堆栈中的自注意子层，以防止位置注意到后面的位置，因为这里仅仅是计算前向概率。这种mask，结合输出嵌入被一个位置偏移的事实，确保了位置i的预测只能依赖于位置小于i的已知输出。")]),t._v(" "),a("h2",{attrs:{id:"attention"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#attention"}},[t._v("#")]),t._v(" Attention")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"50%"},attrs:{src:"/img/AI/NLP/Transformer/2.png"}})]),t._v(" "),a("p",[a("span",{staticClass:"katex-display"},[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",[a("semantics",[a("mrow",[a("mtext",[a("mi",{attrs:{mathvariant:"normal"}},[t._v("A")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("t")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("t")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("e")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("n")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("t")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("i")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("o")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("n")])],1),a("mo",[t._v("(")]),a("mi",[t._v("Q")]),a("mo",{attrs:{separator:"true"}},[t._v(",")]),a("mi",[t._v("K")]),a("mo",{attrs:{separator:"true"}},[t._v(",")]),a("mi",[t._v("V")]),a("mo",[t._v(")")]),a("mo",[t._v("=")]),a("mtext",[a("mi",{attrs:{mathvariant:"normal"}},[t._v("s")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("o")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("f")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("t")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("m")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("a")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("x")])],1),a("mo",[t._v("(")]),a("mfrac",[a("mrow",[a("mi",[t._v("Q")]),a("msup",[a("mi",[t._v("K")]),a("mi",[t._v("T")])],1)],1),a("mrow",[a("msqrt",[a("mrow",[a("msub",[a("mi",[t._v("d")]),a("mi",[t._v("k")])],1)],1)],1)],1)],1),a("mo",[t._v(")")]),a("mi",[t._v("V")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\text{Attention}(Q,K,V)=\\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"strut",staticStyle:{height:"1.5183309999999999em"}}),a("span",{staticClass:"strut bottom",staticStyle:{height:"2.448331em","vertical-align":"-0.9300000000000002em"}}),a("span",{staticClass:"base displaystyle textstyle uncramped"},[a("span",{staticClass:"text mord displaystyle textstyle uncramped"},[a("span",{staticClass:"mord mathrm"},[t._v("A")]),a("span",{staticClass:"mord mathrm"},[t._v("t")]),a("span",{staticClass:"mord mathrm"},[t._v("t")]),a("span",{staticClass:"mord mathrm"},[t._v("e")]),a("span",{staticClass:"mord mathrm"},[t._v("n")]),a("span",{staticClass:"mord mathrm"},[t._v("t")]),a("span",{staticClass:"mord mathrm"},[t._v("i")]),a("span",{staticClass:"mord mathrm"},[t._v("o")]),a("span",{staticClass:"mord mathrm"},[t._v("n")])]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathit"},[t._v("Q")]),a("span",{staticClass:"mpunct"},[t._v(",")]),a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07153em"}},[t._v("K")]),a("span",{staticClass:"mpunct"},[t._v(",")]),a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.22222em"}},[t._v("V")]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"text mord displaystyle textstyle uncramped"},[a("span",{staticClass:"mord mathrm"},[t._v("s")]),a("span",{staticClass:"mord mathrm"},[t._v("o")]),a("span",{staticClass:"mord mathrm",staticStyle:{"margin-right":"0.07778em"}},[t._v("f")]),a("span",{staticClass:"mord mathrm"},[t._v("t")]),a("span",{staticClass:"mord mathrm"},[t._v("m")]),a("span",{staticClass:"mord mathrm"},[t._v("a")]),a("span",{staticClass:"mord mathrm"},[t._v("x")])]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord reset-textstyle displaystyle textstyle uncramped"},[a("span",{staticClass:"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"}),a("span",{staticClass:"mfrac"},[a("span",{staticClass:"vlist"},[a("span",{staticStyle:{top:"0.7472200000000002em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),a("span",{staticClass:"reset-textstyle textstyle cramped"},[a("span",{staticClass:"mord textstyle cramped"},[a("span",{staticClass:"sqrt mord"},[a("span",{staticClass:"sqrt-sign",staticStyle:{top:"-0.017220000000000013em"}},[a("span",{staticClass:"style-wrap reset-textstyle textstyle uncramped"},[t._v("√")])]),a("span",{staticClass:"vlist"},[a("span",{staticStyle:{top:"0em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),a("span",{staticClass:"mord textstyle cramped"},[a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathit"},[t._v("d")]),a("span",{staticClass:"vlist"},[a("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),a("span",{staticClass:"reset-textstyle scriptstyle cramped"},[a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")])])]),a("span",{staticClass:"baseline-fix"},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])]),a("span",{staticStyle:{top:"-0.77722em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),a("span",{staticClass:"reset-textstyle textstyle uncramped sqrt-line"})]),a("span",{staticClass:"baseline-fix"},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),t._v("​")])])])])])]),a("span",{staticStyle:{top:"-0.22999999999999998em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),a("span",{staticClass:"reset-textstyle textstyle uncramped frac-line"})]),a("span",{staticStyle:{top:"-0.677em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),a("span",{staticClass:"reset-textstyle textstyle uncramped"},[a("span",{staticClass:"mord textstyle uncramped"},[a("span",{staticClass:"mord mathit"},[t._v("Q")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07153em"}},[t._v("K")]),a("span",{staticClass:"vlist"},[a("span",{staticStyle:{top:"-0.363em","margin-right":"0.05em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),a("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("T")])])]),a("span",{staticClass:"baseline-fix"},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),a("span",{staticClass:"baseline-fix"},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),t._v("​")])])]),a("span",{staticClass:"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"})]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.22222em"}},[t._v("V")])])])])])]),t._v(" "),a("p",[t._v("Note: attention机制在transformer的用途")]),t._v(" "),a("ul",[a("li",[t._v("在“编码器-解码器注意”层中，q来自前面的decoder层，而k和v来自encoder的输出。这使得译码器中的每个位置都可以参加输入序列中的所有位置。")]),t._v(" "),a("li",[t._v("编码器包含自我注意层。在self-attention层中，所有的k、v和q都来自同一个地方，在本例中，是编码器中前一层的输出。编码器中的每个位置都可以对应上一层的所有位置。")]),t._v(" "),a("li",[t._v("类似地，解码器中的自我注意层允许解码器中的每个位置关注解码器中直到并包括该位置的所有位置。我们需要防止解码器中的信息流向左，以保持自回归特性。我们通过屏蔽(设置为"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",[a("semantics",[a("mrow",[a("mo",[t._v("−")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("∞")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("-\\infty")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"strut",staticStyle:{height:"0.58333em"}}),a("span",{staticClass:"strut bottom",staticStyle:{height:"0.66666em","vertical-align":"-0.08333em"}}),a("span",{staticClass:"base textstyle uncramped"},[a("span",{staticClass:"mord"},[t._v("−")]),a("span",{staticClass:"mord mathrm"},[t._v("∞")])])])]),t._v(")softmax输入中对应非法连接的所有值来实现缩放点积注意。")])]),t._v(" "),a("h2",{attrs:{id:"autoregressive"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#autoregressive"}},[t._v("#")]),t._v(" Autoregressive")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"25%"},attrs:{src:"/img/AI/NLP/Transformer/3.png",alt:"image-20210820141427648"}})]),t._v(" "),a("p",[t._v("decoder将encoder读进来(读进来的方式之后再讲)，然后decoder首先读取一个特殊字符BOS，然后根据这个字符和encoder的输出，去预测一个词。预测完这个词后，将这个词和BOS一起作为decoder的输入，再次进行。")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"25%"},attrs:{src:"/img/AI/NLP/Transformer/4.png",alt:"image-20210820141902552"}})]),t._v(" "),a("p",[t._v("传统的self-attention是每一个词都能够看到序列中的所有词，但是masked self-attention在计算当前词的输出时，只能利用这个词之前的信息，而不能时候之后的信息。这么做是因为decoder是每次计算一个新的词，只能考虑到之前的信息。")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"25%"},attrs:{src:"/img/AI/NLP/Transformer/5.png",alt:"image-20210820142545518"}})]),t._v(" "),a("p",[t._v('增加一个特殊字符"END"，希望将"习"输入进去后，能够成功预测出"END，然后停止计算。'),a("font",{attrs:{color:"yellow"}},[t._v("由于计算当前这个词需要之前的信息，导致了Autoregressive不是并行的。")])],1),t._v(" "),a("h2",{attrs:{id:"cross-attention"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cross-attention"}},[t._v("#")]),t._v(" Cross Attention")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"25%"},attrs:{src:"/img/AI/NLP/Transformer/6.png",alt:"image-20210820143513823"}})]),t._v(" "),a("p",[t._v("encoder 产生的向量做k，v。然后masked self-attention产生向量q，之后将k，q，v进行运算，获得attention的输出。将这个输出输入到FC，进行做分类，获得下一个词的标签。")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"25%"},attrs:{src:"/img/AI/NLP/Transformer/7.png",alt:"image-20210820143936413"}})]),t._v(" "),a("p",[t._v("注意，每次都是将最尾端的词作cross attention，获得attention的输出，然后放进FC里，再进行预测。")]),t._v(" "),a("h2",{attrs:{id:"train"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#train"}},[t._v("#")]),t._v(" Train")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"25%"},attrs:{src:"/img/AI/NLP/Transformer/9.png",alt:"image-20210820145409643"}})]),t._v(" "),a("p",[t._v("其实train是在做分类问题，每次预测下一个词的输入。但是，decoder是能够看到真实词，他在预测下一个词时，是将这之前的词作为decoder的输入，然后跟ecoder的输出一同计算，并最后预测。")]),t._v(" "),a("h3",{attrs:{id:"references"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#references"}},[t._v("#")]),t._v(" References")]),t._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://arxiv.org/abs/1706.03762v5",target:"_blank",rel:"noopener noreferrer"}},[t._v("Attention is all you need."),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.youtube.com/watch?v=n9TlOhRjYoc",target:"_blank",rel:"noopener noreferrer"}},[t._v("李宏毅老师讲解"),a("OutboundLink")],1)])])],1)}),[],!1,null,null,null);s.default=r.exports}}]);