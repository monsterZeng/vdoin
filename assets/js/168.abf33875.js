(window.webpackJsonp=window.webpackJsonp||[]).push([[168],{507:function(t,s,a){"use strict";a.r(s);var e=a(15),i=Object(e.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"what-is-an-activation-function"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#what-is-an-activation-function"}},[t._v("#")]),t._v(" What is an Activation Function?")]),t._v(" "),a("p",[t._v("在神经网络中，激活函数起着重要的作用。激活函数决定了深度学习网络的输出，以及网络训练的准确性和计算效率。如果没有激活函数，那么神经网络就会变成线性计算，拟合能力大大下降。")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://miro.medium.com/max/875/1*5USEYxQuAtscAkAL28fVpw.png"}})]),t._v(" "),a("p",[t._v("神经网络使用非线性激活函数，它可以帮助网络学习复杂的数据，计算和学习几乎代表一个问题的任何函数，并提供准确的预测。")]),t._v(" "),a("h2",{attrs:{id:"sigmoid-function"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sigmoid-function"}},[t._v("#")]),t._v(" Sigmoid Function")]),t._v(" "),a("p",[t._v("Sigmoid是一个非线性激活函数。又称Logistic函数。它是连续而单调的。输出被规范化在0到1的范围内。它是可微的，并给出一个平滑的梯度曲线。在二进制分类中，Sigmoid主要用于输出层之前。")]),t._v(" "),a("p",[a("span",{staticClass:"katex-display"},[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",[a("semantics",[a("mrow",[a("mi",[t._v("σ")]),a("mrow",[a("mo",[t._v("(")]),a("mi",[t._v("x")]),a("mo",[t._v(")")])],1),a("mo",[t._v("=")]),a("mfrac",[a("mrow",[a("mn",[t._v("1")])],1),a("mrow",[a("mn",[t._v("1")]),a("mo",[t._v("+")]),a("mi",[t._v("exp")]),a("mrow",[a("mo",[t._v("(")]),a("mo",[t._v("−")]),a("mi",[t._v("x")]),a("mo",[t._v(")")])],1)],1)],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\sigma{(x)}=\\frac{1}{1+\\exp{(-x)}}\n")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"strut",staticStyle:{height:"1.32144em"}}),a("span",{staticClass:"strut bottom",staticStyle:{height:"2.25744em","vertical-align":"-0.936em"}}),a("span",{staticClass:"base displaystyle textstyle uncramped"},[a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03588em"}},[t._v("σ")]),a("span",{staticClass:"mord displaystyle textstyle uncramped"},[a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathit"},[t._v("x")]),a("span",{staticClass:"mclose"},[t._v(")")])]),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mord reset-textstyle displaystyle textstyle uncramped"},[a("span",{staticClass:"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"}),a("span",{staticClass:"mfrac"},[a("span",{staticClass:"vlist"},[a("span",{staticStyle:{top:"0.686em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),a("span",{staticClass:"reset-textstyle textstyle cramped"},[a("span",{staticClass:"mord textstyle cramped"},[a("span",{staticClass:"mord mathrm"},[t._v("1")]),a("span",{staticClass:"mbin"},[t._v("+")]),a("span",{staticClass:"mop"},[t._v("exp")]),a("span",{staticClass:"mord textstyle cramped"},[a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord"},[t._v("−")]),a("span",{staticClass:"mord mathit"},[t._v("x")]),a("span",{staticClass:"mclose"},[t._v(")")])])])])]),a("span",{staticStyle:{top:"-0.2300000000000001em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),a("span",{staticClass:"reset-textstyle textstyle uncramped frac-line"})]),a("span",{staticStyle:{top:"-0.677em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),a("span",{staticClass:"reset-textstyle textstyle uncramped"},[a("span",{staticClass:"mord textstyle uncramped"},[a("span",{staticClass:"mord mathrm"},[t._v("1")])])])]),a("span",{staticClass:"baseline-fix"},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),a("span",{staticClass:"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"})])])])])])]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://miro.medium.com/max/875/1*ztJZFm0CUlY3HuS-bzmvBw.png"}})]),t._v(" "),a("ul",[a("li",[t._v("Advantage\n"),a("ul",[a("li",[t._v("平滑的梯度，防止输出值的“跳跃”。")]),t._v(" "),a("li",[t._v("输出值限制在0和1之间，对每个神经元的输出进行归一化。")])])]),t._v(" "),a("li",[t._v("Disadvantage\n"),a("ul",[a("li",[t._v("梯度容易消失。因为输出数值很小时，输出就会接近0")]),t._v(" "),a("li",[t._v("函数输出不是以零为中心的")])])])]),t._v(" "),a("h2",{attrs:{id:"tanh-function"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tanh-function"}},[t._v("#")]),t._v(" Tanh function")]),t._v(" "),a("p",[t._v("双曲正切激活函数值的范围为 -1 到 1，导数值介于 0 到 1 之间。它以零为中心。表现优于 sigmoid。它们用于隐藏层的二进制分类。这样做的好处是，在tanh图中，负的输入将被映射为强负的，而零的输入将被映射到零附近。")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://miro.medium.com/max/645/1*opjqHNmb9jU2ay0ovV1bRw.png"}})]),t._v(" "),a("h2",{attrs:{id:"relu-function"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#relu-function"}},[t._v("#")]),t._v(" ReLU function")]),t._v(" "),a("p",[t._v("修正线性单元是深度学习模型中最常用的隐层激活函数。公式非常简单，如果输入是正值，则返回值为0。因此导数也很简单，正值为1，否则为0(因为函数将为0，并将其视为常数，所以导数将为0)，从而解决了梯度消失的问题。取值范围是0到无穷。")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://miro.medium.com/max/875/1*-jDXVBonIa4Goxn4c4QCGQ.png"}})]),t._v(" "),a("ul",[a("li",[t._v("Advantage\n"),a("ul",[a("li",[t._v("当输入为正时，不存在梯度饱和问题。")]),t._v(" "),a("li",[t._v("计算速度快得多。")]),t._v(" "),a("li",[t._v("ReLU函数只有线性关系。")])])]),t._v(" "),a("li",[t._v("Disadvantage\n"),a("ul",[a("li",[t._v("ReLU函数不是一个以0为中心的函数。")]),t._v(" "),a("li",[t._v("当输入为负数时，ReLU 完全处于非活动状态，这意味着一旦输入负数，ReLU 就会死亡。在前向传播过程中，这不是问题，但在反向传播过程中，如果输入负数，则梯度将完全为零，这与 sigmod 函数和 tanh 函数具有相同的问题。")])])])]),t._v(" "),a("h2",{attrs:{id:"leaky-relu-function"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#leaky-relu-function"}},[t._v("#")]),t._v(" Leaky ReLU function")]),t._v(" "),a("p",[t._v("LeakyReLU是ReLU的一个轻微变种。对于正的值，它与ReLU相同，返回相同的输入，对于其他值，提供一个常量0.01带输入。这样做是为了解决垂死的ReLU问题。导数为正时为1，否则为0.01。Leaky ReLU具有ReLU的所有优点。死亡ReLU不会有任何问题。")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://miro.medium.com/max/875/1*xP31TATV4R-IowGxHahrmw.png"}})]),t._v(" "),a("h2",{attrs:{id:"elu-function"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#elu-function"}},[t._v("#")]),t._v(" ELU Function")]),t._v(" "),a("p",[t._v("指数线性单元克服了ReLU失效的问题。非常类似于ReLU，除了负值。如果值为正数，则该函数返回相同的值，否则将得到"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",[a("semantics",[a("mrow",[a("mi",[t._v("α")]),a("mo",[t._v("(")]),a("msup",[a("mi",[t._v("e")]),a("mi",[t._v("x")])],1),a("mo",[t._v("−")]),a("mn",[t._v("1")]),a("mo",[t._v(")")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\alpha(e^x-1)")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"strut",staticStyle:{height:"0.75em"}}),a("span",{staticClass:"strut bottom",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"base textstyle uncramped"},[a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.0037em"}},[t._v("α")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathit"},[t._v("e")]),a("span",{staticClass:"vlist"},[a("span",{staticStyle:{top:"-0.363em","margin-right":"0.05em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),a("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[a("span",{staticClass:"mord mathit"},[t._v("x")])])]),a("span",{staticClass:"baseline-fix"},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),a("span",{staticClass:"mbin"},[t._v("−")]),a("span",{staticClass:"mord mathrm"},[t._v("1")]),a("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("，其中alpha是一个正数常量。当值为正值时，导数为1;当值为负值时，导数为alpha和exp(x)的乘积。取值范围是0到无穷。它是以零为中心的。")]),t._v(" "),a("center",[a("img",{attrs:{src:"https://miro.medium.com/max/694/1*ynBuZCRSWpEsFWIo3api6A.png"}})]),t._v(" "),a("ul",[a("li",[t._v("Advantage\n"),a("ul",[a("li",[t._v("No Dead ReLU issues")]),t._v(" "),a("li",[t._v("The mean of the output is close to 0, zero-centered")])])]),t._v(" "),a("li",[t._v("Disadvantage\n"),a("ul",[a("li",[t._v("Problem with ELU- slightly more computationally intensive.")])])])]),t._v(" "),a("h2",{attrs:{id:"prelu-parametric-relu"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#prelu-parametric-relu"}},[t._v("#")]),t._v(" PRelu (Parametric ReLU)")]),t._v(" "),a("p",[t._v("参数化整流线性单元是ReLU和Leaky ReLU的变种，负值计算为alpha*输入。与Leaky ReLU的alpha=0.01不同，在PReLU中alpha值将通过放置不同的值反向传播来学习，因此将提供最佳的学习曲线。")]),t._v(" "),a("center",[a("img",{attrs:{src:"https://miro.medium.com/max/593/1*eqXJctg7pcULUDxb1ksakw.jpeg"}})]),t._v(" "),a("ul",[a("li",[t._v("if aᵢ=0, f becomes ReLU")]),t._v(" "),a("li",[t._v("if aᵢ>0, f becomes leaky ReLU")]),t._v(" "),a("li",[t._v("if aᵢ is a learnable parameter, f becomes PReLU")])]),t._v(" "),a("h2",{attrs:{id:"swish-a-self-gated-function"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#swish-a-self-gated-function"}},[t._v("#")]),t._v(" Swish (A Self-Gated) Function")]),t._v(" "),a("p",[t._v("Swish是一种ReLU功能。它是一个self-gated函数，只需要输入，不需要其他参数。公式"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",[a("semantics",[a("mrow",[a("mi",[t._v("y")]),a("mo",[t._v("=")]),a("mi",[t._v("x")]),a("mo",[t._v("∗")]),a("mtext",[a("mi",{attrs:{mathvariant:"normal"}},[t._v("s")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("i")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("g")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("m")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("o")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("i")]),a("mi",{attrs:{mathvariant:"normal"}},[t._v("d")])],1),a("mo",[t._v("(")]),a("mi",[t._v("x")]),a("mo",[t._v(")")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("y=x*\\text{sigmoid}(x)")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"strut",staticStyle:{height:"0.75em"}}),a("span",{staticClass:"strut bottom",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"base textstyle uncramped"},[a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mord mathit"},[t._v("x")]),a("span",{staticClass:"mbin"},[t._v("∗")]),a("span",{staticClass:"text mord textstyle uncramped"},[a("span",{staticClass:"mord mathrm"},[t._v("s")]),a("span",{staticClass:"mord mathrm"},[t._v("i")]),a("span",{staticClass:"mord mathrm",staticStyle:{"margin-right":"0.01389em"}},[t._v("g")]),a("span",{staticClass:"mord mathrm"},[t._v("m")]),a("span",{staticClass:"mord mathrm"},[t._v("o")]),a("span",{staticClass:"mord mathrm"},[t._v("i")]),a("span",{staticClass:"mord mathrm"},[t._v("d")])]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathit"},[t._v("x")]),a("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("主要用于LSTM。零中心，解决了激活失效的问题。具有平滑性，有助于泛化和优化。")]),t._v(" "),a("center",[a("img",{attrs:{src:"https://miro.medium.com/max/750/1*O3McGKu_OoRja2ROBhQu6w.png"}})]),t._v(" "),a("p",[t._v("缺点:计算能力强，仅在神经网络层数超过40层时使用。")]),t._v(" "),a("h2",{attrs:{id:"softmax-function"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#softmax-function"}},[t._v("#")]),t._v(" Softmax function")]),t._v(" "),a("p",[t._v("Softmax激活函数返回输入的概率作为输出。概率将用于找出目标类。最终输出将是概率最高的那个。所有这些概率的总和必须等于1。这主要用于分类问题，最好用于多类分类。")]),t._v(" "),a("center",[a("img",{attrs:{src:"https://miro.medium.com/max/875/1*DwPfsGHZcguvr9vEBzlbUA.png"}})]),t._v(" "),a("p",[t._v("Softmax不适用于线性可分的数据。")]),t._v(" "),a("h2",{attrs:{id:"softplus-function"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#softplus-function"}},[t._v("#")]),t._v(" Softplus function")]),t._v(" "),a("p",[t._v("求0的导数在数学上是不可能的。由于这个问题，大多数激活函数都失败了。通过软加激活函数克服了这一问题。公式y = ln(1 + exp(x))它类似于ReLU。在自然流畅。范围从0到无穷大。")]),t._v(" "),a("center",[a("img",{attrs:{src:"https://miro.medium.com/max/370/1*FSGAnVbjtrVd05fSAI_11w.png"}})]),t._v(" "),a("p",[t._v("缺点:由于它的光滑性和无界性，softplus会在更大程度上破坏激活。")]),t._v(" "),a("h3",{attrs:{id:"reference"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#reference"}},[t._v("#")]),t._v(" Reference:")]),t._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://2809ayushic.medium.com/types-of-activation-functions-fc9e71c2d991",target:"_blank",rel:"noopener noreferrer"}},[t._v("Types of activation function"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://liam.page/2018/04/17/zero-centered-active-function/",target:"_blank",rel:"noopener noreferrer"}},[t._v("以零为中心的激活函数的好处"),a("OutboundLink")],1)])])],1)}),[],!1,null,null,null);s.default=i.exports}}]);